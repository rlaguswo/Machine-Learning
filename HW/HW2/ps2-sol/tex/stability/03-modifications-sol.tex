\begin{answer}
  \begin{enumerate}[label=\roman*.]
    \item No.
    \item No / may help. Eventually updates will become arbitrarily small.
    Still, the norm of theta can end up being very big. You may also end up
    underfitting if you aggressively decrease the learning rate.
    \item No. This will not affect whether the examples are linearly
    separable.
    \item Yes. This penalizes large norm of theta and makes the training
    converge on all kinds of data.
    \item No / may help if the noise happens to make the data linearly
    inseparable.
  \end{enumerate}
\end{answer}
